{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7214938-e4d7-4c05-9327-24aef08004fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import einops\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d316d1f-35b4-461a-b648-6872de118ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data_paths': '/home/sz9jt/data/t1w_processed/outputs_8mm/2025-01-28T00_42_41_medil_reduced_CT_pretrained/train_val_encodings', 'transform': 'None', 'fixed_scale': 0.13, 'output_shape': [32, 32, 32], 'if_sample': True, 'scale_each_ch': True, 'ch1_fixed_scale': 0.25, 'ch2_fixed_scale': 0.133, 'ch3_fixed_scale': 0.133, 'ch4_fixed_scale': 0.167, 'ch5_fixed_scale': 0.25, 'ch6_fixed_scale': 0.25, 'ch7_fixed_scale': 0.25, 'ch8_fixed_scale': 0.25}\n",
      "We should scale each dimension\n",
      "Using scale: [0.25, 0.133, 0.133, 0.167, 0.25, 0.25, 0.25, 0.25]\n"
     ]
    }
   ],
   "source": [
    "src_path = Path(\"/home/sz9jt/data/generative_brain/diffusion/diffusion_medil_reduced_CT_pretrained_channelwise\")\n",
    "out_path = src_path / \"sample-unscaled\"\n",
    "out_path.mkdir(exist_ok=True)\n",
    "config = OmegaConf.load(src_path / \"config.yaml\")\n",
    "\n",
    "print(config.dataset.params)\n",
    "\n",
    "if config.dataset.params.scale_each_ch:\n",
    "    print(\"We should scale each dimension\")\n",
    "    scale = [\n",
    "             config.dataset.params.ch1_fixed_scale,\n",
    "             config.dataset.params.ch2_fixed_scale,\n",
    "             config.dataset.params.ch3_fixed_scale,\n",
    "             config.dataset.params.ch4_fixed_scale,\n",
    "             config.dataset.params.ch5_fixed_scale,\n",
    "             config.dataset.params.ch6_fixed_scale,\n",
    "             config.dataset.params.ch7_fixed_scale,\n",
    "             config.dataset.params.ch8_fixed_scale,\n",
    "            ]\n",
    "else:\n",
    "    print(\"Global scale\")\n",
    "    scale = config.dataset.params.fixed_scale\n",
    "    \n",
    "print(\"Using scale:\", scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c4b89440-56da-4f6b-8606-104430e63d83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  10.    0.   -0. -155.]\n",
      " [   0.   10.   -0. -155.]\n",
      " [  -0.    0.   10. -155.]\n",
      " [   0.    0.    0.    1.]]\n",
      "[[  10.    0.   -0. -155.]\n",
      " [   0.   10.   -0. -155.]\n",
      " [  -0.    0.   10. -155.]\n",
      " [   0.    0.    0.    1.]]\n",
      "[[  10.    0.   -0. -155.]\n",
      " [   0.   10.   -0. -155.]\n",
      " [  -0.    0.   10. -155.]\n",
      " [   0.    0.    0.    1.]]\n",
      "[[  10.    0.   -0. -155.]\n",
      " [   0.   10.   -0. -155.]\n",
      " [  -0.    0.   10. -155.]\n",
      " [   0.    0.    0.    1.]]\n",
      "[[  10.    0.   -0. -155.]\n",
      " [   0.   10.   -0. -155.]\n",
      " [  -0.    0.   10. -155.]\n",
      " [   0.    0.    0.    1.]]\n"
     ]
    }
   ],
   "source": [
    "encoding_path = config.dataset.params.data_paths\n",
    "tmp = os.listdir(encoding_path)\n",
    "\n",
    "for i in range(5):\n",
    "    img = nib.load(f\"{encoding_path}/{tmp[i]}\")\n",
    "    print(img.affine)\n",
    "    affine = img.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2433452b-1fd3-430b-81a2-bd3f5c2363d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_epoch1000_fixscale0.13_0.npy\n",
      "(1, 8, 32, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "samples = os.listdir(src_path / \"sample-ckpt1000\")\n",
    "samples.sort()\n",
    "print(samples[0])\n",
    "img = np.load(src_path / \"sample-ckpt1000\" / samples[0])\n",
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7bfb4ee5-fa54-46f7-91d2-665817a39cd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_epoch1000_fixscale0.13_0.npy\n",
      "Channelwise scale\n"
     ]
    }
   ],
   "source": [
    "# For brain\n",
    "samples = os.listdir(src_path / \"sample-ckpt1000\")\n",
    "samples.sort()\n",
    "print(samples[0])\n",
    "\n",
    "    \n",
    "def scale_samples(samples, scale):\n",
    "    for item in samples:\n",
    "        out_name = item.replace(\".npy\", \".nii.gz\")\n",
    "        img = np.load(src_path / \"sample-ckpt1000\" / item)\n",
    "        img = img.squeeze()\n",
    "        img = np.transpose(img, (1, 2, 3, 0))\n",
    "        if isinstance(scale, int):\n",
    "            img = img / scale\n",
    "        elif isinstance(scale, list):\n",
    "            assert img.shape[-1] == len(scale)\n",
    "            tmp = np.array(scale)[None, None, None, ...]\n",
    "            img = img / tmp\n",
    "        else:\n",
    "            raise ValueError(\"Wrong scale_type\")\n",
    "\n",
    "        nifti_img = nib.Nifti1Image(img, affine=affine)\n",
    "        nib.save(nifti_img, out_path / out_name)\n",
    "        \n",
    "\n",
    "if isinstance(scale, int):\n",
    "    print(\"Global scale\")    \n",
    "elif isinstance(scale, list):\n",
    "    print(\"Channelwise scale\")\n",
    "scale_samples(samples, scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb50e30c-fa55-4655-9cbe-5104cd548d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "(32, 32, 32, 8) -7.30332216822115 6.721348690807371\n",
      "-3.9100406169891357 3.864105701446533\n",
      "-7.30332216822115 4.662463091369858\n",
      "-4.9443482456350685 6.721348690807371\n"
     ]
    }
   ],
   "source": [
    "out = os.listdir(out_path)\n",
    "print(len(out))\n",
    "\n",
    "data = nib.load(out_path / out[0]).get_fdata()\n",
    "\n",
    "print(data.shape, data.min(), data.max())\n",
    "print(data[..., 0].min(), data[..., 0].max())\n",
    "print(data[..., 1].min(), data[..., 1].max())\n",
    "print(data[..., 2].min(), data[..., 2].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687ce456-cff3-4274-9b31-9915f1c925d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mrinr",
   "language": "python",
   "name": "mrinr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
